@article{Shridhar2020,
 abstract = {This article presents INGRESS, a robot system that follows human natural language instructions to pick and place everyday objects. The key question here is to ground referring expressions: understand expressions about objects and their relationships from image and natural language inputs. INGRESS allows unconstrained object categories and rich language expressions. Further, it asks questions to clarify ambiguous referring expressions interactively. To achieve these, we take the approach of grounding by generation and propose a two-stage neural-network model for grounding. The first stage uses a neural network to generate visual descriptions of objects, compares them with the input language expressions, and identifies a set of candidate objects. The second stage uses another neural network to examine all pairwise relations between the candidates and infers the most likely referred objects. The same neural networks are used for both grounding and question generation for disambiguation. Experiments show that INGRESS outperformed a state-of-the-art method on the RefCOCO dataset and in robot experiments with humans. The INGRESS source code is available at https://github.com/MohitShridhar/ingress.},
 author = {Shridhar, Mohit and Mittal, Dixant and Hsu, David},
 doi = {10.1177/0278364919897133},
 file = {:Users/dixant/Mendeley/Shridhar, Mittal, Hsu - 2020 - INGRESS Interactive visual grounding of referring expressions.pdf:pdf},
 issn = {17413176},
 journal = {International Journal of Robotics Research},
 keywords = {Natural language grounding,disambiguation,humanâ€“robot interaction},
 month = {mar},
 number = {2-3},
 pages = {217--232},
 publisher = {SAGE Publications Ltd STM},
 title = {INGRESS: Interactive visual grounding of referring expressions},
 volume = {39},
 year = {2020}
}

